{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìå 1Ô∏è‚É£ Extract & Clean Text from PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF for PDF processing\n",
    "import os\n",
    "import json\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Extracted and cleaned text from 55 reports. Saved to clean_reports.json.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\" Extract raw text from a PDF file \"\"\"\n",
    "    text = \"\"\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        for page in doc:\n",
    "            text += page.get_text(\"text\") + \"\\n\"\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error reading {pdf_path}: {e}\")\n",
    "    return text\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\" Clean extracted text by removing unwanted characters, extra spaces, and symbols \"\"\"\n",
    "    text = re.sub(r'\\n+', '\\n', text)  # Remove extra newlines\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
    "    text = re.sub(r'[^A-Za-z0-9ÿ£-Ÿä .,ÿõ:ÿü!-]', '', text)  # Keep only meaningful characters\n",
    "    text = re.sub(r'\\bPage \\d+\\b', '', text, flags=re.IGNORECASE)  # Remove \"Page X\" footers\n",
    "    text = re.sub(r'\\bFigure \\d+\\b', '', text, flags=re.IGNORECASE)  # Remove \"Figure X\" references\n",
    "    text = re.sub(r'\\bTable \\d+\\b', '', text, flags=re.IGNORECASE)  # Remove \"Table X\" references\n",
    "    return text.strip()\n",
    "\n",
    "def process_pdfs_in_folder(folder_path, output_json=\"clean_reports.json\"):\n",
    "    \"\"\" Extract and clean text from all PDFs in a folder and save to JSON \"\"\"\n",
    "    pdf_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith(\".pdf\")]\n",
    "\n",
    "    reports = []\n",
    "    for pdf_file in pdf_files:\n",
    "        raw_text = extract_text_from_pdf(pdf_file)\n",
    "        cleaned_text = clean_text(raw_text)\n",
    "        if cleaned_text:\n",
    "            reports.append({\"file\": os.path.basename(pdf_file), \"text\": cleaned_text})\n",
    "\n",
    "    # Save extracted text to JSON\n",
    "    with open(output_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(reports, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"‚úÖ Extracted and cleaned text from {len(reports)} reports. Saved to {output_json}.\")\n",
    "\n",
    "# Run the script on the PDF folder\n",
    "process_pdfs_in_folder(r\"C:\\Users\\rkhm3\\Desktop\\GeoKnowlogy_Dataset\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìå 2Ô∏è‚É£ Convert Cleaned Text into LLM Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Prepared 37836 samples for training. Saved to training_data.json.\n"
     ]
    }
   ],
   "source": [
    "def prepare_data_for_training(json_file, output_file=\"training_data.json\"):\n",
    "    \"\"\" Convert cleaned text into structured LLM training data \"\"\"\n",
    "    with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        reports = json.load(f)\n",
    "\n",
    "    dataset = []\n",
    "    for report in reports:\n",
    "        text = report[\"text\"]\n",
    "        sections = re.split(r'\\.\\s+', text)  # Split text into sentences\n",
    "\n",
    "        for section in sections:\n",
    "            if len(section.strip()) > 50:  # Ignore very short lines\n",
    "                dataset.append({\n",
    "                    \"instruction\": \"Summarize this geological information:\",\n",
    "                    \"input\": section,\n",
    "                    \"output\": \"Summary of this section...\"\n",
    "                })\n",
    "\n",
    "    # Save formatted training data\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(dataset, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"‚úÖ Prepared {len(dataset)} samples for training. Saved to {output_file}.\")\n",
    "\n",
    "# Convert extracted data into structured training data\n",
    "prepare_data_for_training(\"clean_reports.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìå 3Ô∏è‚É£ Fine-Tune the LLM on Your Data (LoRA for Efficiency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f596b120da5142bcae274c647793a491",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu and disk.\n"
     ]
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "model_name = \"ALLaM-AI/ALLaM-7B-Instruct-preview\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply LoRA for memory-efficient fine-tuning\n",
    "lora_config = LoraConfig(r=8, lora_alpha=32, target_modules=[\"q_proj\", \"v_proj\"],\n",
    "                         lora_dropout=0.1, bias=\"none\", task_type=\"CAUSAL_LM\")\n",
    "model = get_peft_model(model, lora_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load training data\n",
    "with open(\"training_data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    train_data = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training settings\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./trained_model\",\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    save_steps=500,\n",
    "    logging_steps=100\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìå 4Ô∏è‚É£ Deploy the Trained Model as an API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Load trained model\n",
    "model_name = \"./trained_model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\", device_map=\"auto\")\n",
    "\n",
    "@app.get(\"/ask\")\n",
    "def ask(question: str):\n",
    "    \"\"\" API to interact with the trained model \"\"\"\n",
    "    inputs = tokenizer(question, return_tensors=\"pt\")\n",
    "    output = model.generate(**inputs, max_length=500)\n",
    "    return {\"answer\": tokenizer.decode(output[0], skip_special_tokens=True)}\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
